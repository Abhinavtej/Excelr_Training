{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeZUihzorblNcTiERekYbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinavtej/Excelr_Training/blob/main/Assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 1 - Assignment\n",
        "\n",
        "Write a program in Jupyter Notebook to declare variables of different data types (integer, float, string, and boolean).\n",
        "Print each variable and its type.\n",
        "'''\n",
        "\n",
        "integer_variable = 10\n",
        "float_variable = 3.14\n",
        "string_variable = \"Hello, world!\"\n",
        "boolean_variable = True\n",
        "\n",
        "print(f\"{integer_variable = }, Type: {type(integer_variable)}\")\n",
        "print(f\"{float_variable = }, Type: {type(float_variable)}\")\n",
        "print(f\"{string_variable = }, Type: {type(string_variable)}\")\n",
        "print(f\"{boolean_variable = }, Type: {type(boolean_variable)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFgHl4TBHqC9",
        "outputId": "1fbc3080-db2e-4b1f-f189-e69f5f676ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "integer_variable = 10, Type: <class 'int'>\n",
            "float_variable = 3.14, Type: <class 'float'>\n",
            "string_variable = 'Hello, world!', Type: <class 'str'>\n",
            "boolean_variable = True, Type: <class 'bool'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 2 - Assignment\n",
        "\n",
        "Create a List, tuple and Dictionary with 5 elements in it and how to access few elements based on the index.\n",
        "Try  with different examples\n",
        "'''\n",
        "\n",
        "my_list = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n",
        "print(my_list[0])\n",
        "print(my_list[2])\n",
        "print(my_list[-1])\n",
        "print()\n",
        "my_tuple = (\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\")\n",
        "print(my_tuple[0])\n",
        "print(my_tuple[2])\n",
        "print(my_tuple[-1])\n",
        "print()\n",
        "my_dict = {\"name\": \"Abhinav\", \"age\": 20, \"city\": \"Hyderabad\", \"college\": \"Malla Reddy University\", \"course\": \"BTech\"}\n",
        "print(my_dict[\"name\"])\n",
        "print(my_dict[\"age\"])\n",
        "print(my_dict[\"city\"])\n",
        "print(my_dict[\"college\"])\n",
        "print(my_dict[\"course\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3BTdtOvIOQk",
        "outputId": "54d78e82-0610-4c75-9b6a-f2e7f4b5b8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\n",
            "cherry\n",
            "elderberry\n",
            "\n",
            "apple\n",
            "cherry\n",
            "elderberry\n",
            "\n",
            "Abhinav\n",
            "20\n",
            "Hyderabad\n",
            "Malla Reddy University\n",
            "BTech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 3 - Assignment\n",
        "\n",
        "Write a Python program that takes a student's marks in three subjects as input.\n",
        "  If the average is greater than or equal to 90, print \"Grade: A\".\n",
        "  If the average is between 80 and 89, print \"Grade: B\".\n",
        "  If the average is between 70 and 79, print \"Grade: C\".\n",
        "  Otherwise, print \"Grade: Fail\".\n",
        "'''\n",
        "\n",
        "subject1 = float(input(\"Enter marks for subject 1: \"))\n",
        "subject2 = float(input(\"Enter marks for subject 2: \"))\n",
        "subject3 = float(input(\"Enter marks for subject 3: \"))\n",
        "\n",
        "average = (subject1 + subject2 + subject3) / 3\n",
        "\n",
        "if average >= 90:\n",
        "    grade = \"A\"\n",
        "elif average >= 80:\n",
        "    grade = \"B\"\n",
        "elif average >= 70:\n",
        "    grade = \"C\"\n",
        "else:\n",
        "    grade = \"Fail\"\n",
        "\n",
        "print(f\"Grade: {grade}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoojUDdvI7PW",
        "outputId": "37b187b8-007c-4aee-ed99-cf2279949e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter marks for subject 1: 100\n",
            "Enter marks for subject 2: 99\n",
            "Enter marks for subject 3: 98\n",
            "Grade: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMbIZ_PEHlKK",
        "outputId": "b51191bc-78e0-46ac-f290-5a4823472f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter n value: 10\n",
            "Sum of Even Numbers from 1 to 10: 30\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Day 4 - Assignment\n",
        "Write a Python program to calculate the sum of all even numbers between 1 and a given positive integer n\n",
        "'''\n",
        "\n",
        "def sum_of_even_numbers(n):\n",
        "  sum = 0\n",
        "  for i in range(1, n + 1):\n",
        "    if i % 2 == 0:\n",
        "      sum += i\n",
        "  return sum\n",
        "\n",
        "n = int(input(\"Enter n value: \"))\n",
        "print(f\"Sum of Even Numbers from 1 to {n}: {sum_of_even_numbers(n)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 5 - Assignment\n",
        "\n",
        "Write a Python program to calculate the frequency of each word in a given text.\n",
        "Print the words and their corresponding counts\n",
        "'''\n",
        "\n",
        "def word_frequency(corpus):\n",
        "    words = corpus.lower().split()\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "    return word_counts\n",
        "\n",
        "\n",
        "def print_word_counts(word_counts):\n",
        "    for word, count in word_counts.items():\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "\n",
        "corpus = input(\"Enter Corpus: \")\n",
        "frequencies = word_frequency(corpus)\n",
        "print_word_counts(frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNZjhXJChE65",
        "outputId": "9e1f003c-c571-4ca1-ef21-fb8446d780dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Corpus: Natural language processing is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\n",
            "natural: 1\n",
            "language: 1\n",
            "processing: 1\n",
            "is: 1\n",
            "a: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "technology: 1\n",
            "that: 1\n",
            "gives: 1\n",
            "computers: 1\n",
            "the: 1\n",
            "ability: 1\n",
            "to: 1\n",
            "interpret,: 1\n",
            "manipulate,: 1\n",
            "and: 1\n",
            "comprehend: 1\n",
            "human: 1\n",
            "language.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 6 - Assignment\n",
        "\n",
        "Write a Python program to using NLTK and Spacy\n",
        "  Convert text to lowercase.\n",
        "  Remove stopwords using NLTK\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [token.text for token in doc if token.text not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "text = input(\"Enter Corpus:\")\n",
        "processed_text = preprocess_text(text)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XMtYx1ShG1I",
        "outputId": "8ee2b2b6-75d7-4016-80bb-0ca0fb2bfdce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Corpus:Natural language processing is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\n",
            "natural language processing machine learning technology gives computers ability interpret , manipulate , comprehend human language .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 7 - Assignment\n",
        "\n",
        "Write a Python script that:\n",
        "  Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc.\n",
        "'''\n",
        "\n",
        "import gensim\n",
        "import docx2txt\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "\n",
        "context = docx2txt.process(\"CORPUS.docx\")\n",
        "processed_docs = preprocess(context)\n",
        "print(processed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEW7pDqQhHUL",
        "outputId": "24c1babe-e25d-4eac-c524-22e063885efb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['human', 'communic', 'form', 'languag', 'text', 'speech', 'interact', 'comput', 'human', 'comput', 'need', 'understand', 'natur', 'languag', 'human', 'natur', 'languag', 'process', 'make', 'comput', 'learn', 'understand', 'analyz', 'manipul', 'interpret', 'natur', 'human', 'languag', 'stand', 'natur', 'languag', 'process', 'scienc', 'human', 'languag', 'linguist', 'artifici', 'intellig', 'process', 'natur', 'languag', 'requir', 'want', 'intellig', 'like', 'robot', 'perform', 'instruct', 'want', 'hear', 'decis', 'dialogu', 'base', 'clinic', 'expert', 'abil', 'machin', 'interpret', 'human', 'languag', 'core', 'applic', 'chatbot', 'email', 'classif', 'spam', 'filter', 'search', 'engin', 'grammar', 'checker', 'voic', 'assist', 'social', 'languag', 'translat', 'input', 'output', 'speech', 'write', 'text', 'compon', 'natur', 'languag', 'understand', 'natur', 'languag', 'generat', 'natur', 'languag', 'understand', 'involv', 'transform', 'human', 'languag', 'machin', 'readabl', 'format', 'help', 'machin', 'understand', 'analyz', 'human', 'languag', 'extract', 'text', 'larg', 'data', 'keyword', 'emot', 'relat', 'semant', 'natur', 'languag', 'generat', 'act', 'translat', 'convert', 'computer', 'data', 'natur', 'languag', 'represent', 'main', 'involv', 'text', 'plan', 'sentenc', 'plan', 'text', 'realize']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Day 8 - Assignment\n",
        "\n",
        "Write a Python script that:\n",
        "  Tokenizes a sample paragraph into words and sentences.\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "\n",
        "def tokenize_text(paragraph):\n",
        "    sentences = nltk.sent_tokenize(paragraph)\n",
        "    words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "    return sentences, words\n",
        "\n",
        "context = docx2txt.process(\"CORPUS.docx\")\n",
        "sentences, words = tokenize_text(context)\n",
        "\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nWords:\")\n",
        "for sentence_words in words:\n",
        "    print(sentence_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwakIKMHhIDp",
        "outputId": "1571ac40-4557-4579-ff2b-be4bfe19165b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "Humans communicate through some form of language either by text or speech.\n",
            "To make interactions between computers and humans, computers need to understand natural languages used by humans.\n",
            "Natural language processing is all about making computers learn, understand, analyze, manipulate and interpret natural(human) languages.\n",
            "NLP stands for Natural Language Processing, which is a part of Computer Science, Human languages or Linguistics, and Artificial Intelligence.\n",
            "Processing of Natural Language is required when you want an intelligent system like robot to perform as per your instructions, when you want to hear decision from a dialogue based clinical expert system, etc.\n",
            "The ability of machines to interpret human language is now at the core of many applications that we use every day - chatbots, Email classification and spam filters, search engines, grammar checkers, voice assistants, and social language translators.\n",
            "The input and output of an NLP system can be Speech or Written Text.\n",
            "There are two components of NLP, Natural Language Understanding (NLU)and Natural Language Generation (NLG).\n",
            "Natural Language Understanding (NLU) which involves transforming human language into a machine-readable format.\n",
            "It helps the machine to understand and analyze human language by extracting the text from large data such as keywords, emotions, relations, and semantics.\n",
            "Natural Language Generation (NLG) acts as a translator that converts the computerized data into natural language representation.\n",
            "It mainly involves Text planning, Sentence planning, and Text realization.\n",
            "\n",
            "Words:\n",
            "['Humans', 'communicate', 'through', 'some', 'form', 'of', 'language', 'either', 'by', 'text', 'or', 'speech', '.']\n",
            "['To', 'make', 'interactions', 'between', 'computers', 'and', 'humans', ',', 'computers', 'need', 'to', 'understand', 'natural', 'languages', 'used', 'by', 'humans', '.']\n",
            "['Natural', 'language', 'processing', 'is', 'all', 'about', 'making', 'computers', 'learn', ',', 'understand', ',', 'analyze', ',', 'manipulate', 'and', 'interpret', 'natural', '(', 'human', ')', 'languages', '.']\n",
            "['NLP', 'stands', 'for', 'Natural', 'Language', 'Processing', ',', 'which', 'is', 'a', 'part', 'of', 'Computer', 'Science', ',', 'Human', 'languages', 'or', 'Linguistics', ',', 'and', 'Artificial', 'Intelligence', '.']\n",
            "['Processing', 'of', 'Natural', 'Language', 'is', 'required', 'when', 'you', 'want', 'an', 'intelligent', 'system', 'like', 'robot', 'to', 'perform', 'as', 'per', 'your', 'instructions', ',', 'when', 'you', 'want', 'to', 'hear', 'decision', 'from', 'a', 'dialogue', 'based', 'clinical', 'expert', 'system', ',', 'etc', '.']\n",
            "['The', 'ability', 'of', 'machines', 'to', 'interpret', 'human', 'language', 'is', 'now', 'at', 'the', 'core', 'of', 'many', 'applications', 'that', 'we', 'use', 'every', 'day', '-', 'chatbots', ',', 'Email', 'classification', 'and', 'spam', 'filters', ',', 'search', 'engines', ',', 'grammar', 'checkers', ',', 'voice', 'assistants', ',', 'and', 'social', 'language', 'translators', '.']\n",
            "['The', 'input', 'and', 'output', 'of', 'an', 'NLP', 'system', 'can', 'be', 'Speech', 'or', 'Written', 'Text', '.']\n",
            "['There', 'are', 'two', 'components', 'of', 'NLP', ',', 'Natural', 'Language', 'Understanding', '(', 'NLU', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', '.']\n",
            "['Natural', 'Language', 'Understanding', '(', 'NLU', ')', 'which', 'involves', 'transforming', 'human', 'language', 'into', 'a', 'machine-readable', 'format', '.']\n",
            "['It', 'helps', 'the', 'machine', 'to', 'understand', 'and', 'analyze', 'human', 'language', 'by', 'extracting', 'the', 'text', 'from', 'large', 'data', 'such', 'as', 'keywords', ',', 'emotions', ',', 'relations', ',', 'and', 'semantics', '.']\n",
            "['Natural', 'Language', 'Generation', '(', 'NLG', ')', 'acts', 'as', 'a', 'translator', 'that', 'converts', 'the', 'computerized', 'data', 'into', 'natural', 'language', 'representation', '.']\n",
            "['It', 'mainly', 'involves', 'Text', 'planning', ',', 'Sentence', 'planning', ',', 'and', 'Text', 'realization', '.']\n"
          ]
        }
      ]
    }
  ]
}